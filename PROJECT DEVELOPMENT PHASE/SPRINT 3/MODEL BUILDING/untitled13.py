# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UrwAA8N6Ao_hnIPSr-VFgvXtKx5hbyzM
"""

from google.colab import drive
drive.mount('/content/drive')

from tensorflow.keras.layers import Dense, Flatten, Input
from tensorflow.keras.models import Model
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img
from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input
from glob import glob
import numpy as np
import matplotlib.pyplot as plt
imageSize = [224, 224]

trainPath = r"/content/drive/MyDrive/Car damage/Car damage/body/training"

testPath = r"/content/drive/MyDrive/Car damage/Car damage/body/validation"

# adding preprocessing layers to the front of vgg

vgg = VGG16(input_shape=imageSize + [3], weights='imagenet',include_top=False)

# don't train existing weights
for layer in vgg.layers:
  layer.trainable = False
# our layers - you can add more if you want
x = Flatten()(vgg.output)
prediction = Dense(3, activation='softmax')(x)
# create a model object
model = Model(inputs=vgg.input, outputs=prediction)
# view the structure of the model
model.summary()
Model: "model"

# tell the model what cost and optimization method to use
model.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory(trainPath,
                                                 target_size = (224, 224),
                                                 batch_size = 10,
                                                 class_mode = 'categorical')

test_set = test_datagen.flow_from_directory(testPath,
                                            target_size = (224, 224),
                                            batch_size = 10,
                                            class_mode = 'categorical')

import sys
# fit the model
r = model.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=10,
  steps_per_epoch=979//10,
  validation_steps=171//10)

#save the model
model.save('body.h5')
#import load_model class for loading h5 file
from tensorflow.keras.models import load_model
#import image class to process the images
from tensorflow.keras.preprocessing import image
from tensorflow.keras.applications.inception_v3 import preprocess_input
import numpy as np
#load one random image from local system
img=image.load_img(r'/content/drive/MyDrive/Car damage/Car damage/body/training/00-front/0005.JPEG',target_size=(224,224))
#convert image to array format
x=image.img_to_array(img)
import numpy as np
x=np.expand_dims(x,axis=0)
img_data=preprocess_input(x)
img_data.shape
(1, 224, 224, 3)
img_data.shape
(1, 224, 224, 3)
model.predict(img_data)

output=np.argmax(model.predict(img_data), axis=1)
output

imageSize = [224, 224]

trainPath = r"/content/drive/MyDrive/Car damage/Car damage/level/training"

testPath = r"/content/drive/MyDrive/Car damage/Car damage/level/validation"
vgg1 = VGG16(input_shape=imageSize + [3], weights='imagenet',include_top=False)
for layer in vgg1.layers:
  layer.trainable = False
# our layers - you can add more if you want
x = Flatten()(vgg1.output)
prediction = Dense(3, activation='softmax')(x)
# create a model object
model1 = Model(inputs=vgg1.input, outputs=prediction)
# tell the model what cost and optimization method to use
model1.compile(
  loss='categorical_crossentropy',
  optimizer='adam',
  metrics=['accuracy']
)
train_datagen = ImageDataGenerator(rescale = 1./255,
                                   shear_range = 0.2,
                                   zoom_range = 0.2,
                                   horizontal_flip = True)

test_datagen = ImageDataGenerator(rescale = 1./255)
training_set = train_datagen.flow_from_directory(trainPath,
                                                 target_size = (224, 224),
                                                 batch_size = 10,
                                                 class_mode = 'categorical')

test_set = test_datagen.flow_from_directory(testPath,
                                            target_size = (224, 224),
                                            batch_size = 10,
                                            class_mode = 'categorical')

r = model1.fit_generator(
  training_set,
  validation_data=test_set,
  epochs=10,
  steps_per_epoch=979//10,
  validation_steps=171//10)

#save the model
model.save('level.h5')





